# Hallucination Handler for Robotics

## Overview
Hallucination handling in robotics refers to the detection, validation, and correction of false or unreliable information generated by AI systems that control or assist robotic operations. This includes false sensor readings, incorrect environmental assumptions, invalid planning decisions, and erroneous interpretations of the physical world that could lead to unsafe robot behavior.

## Key Concepts
- **Reality Verification**: Cross-checking AI-generated information against physical reality
- **Sensor Fusion Validation**: Combining multiple sensor inputs to validate information
- **Uncertainty Quantification**: Measuring and managing uncertainty in AI decisions
- **Consistency Checking**: Ensuring AI outputs are consistent with known constraints
- **Fallback Mechanisms**: Safe alternatives when AI information is unreliable
- **Ground Truth Validation**: Using known physical constraints to verify AI outputs

## Essential Hallucination Handling Techniques

### 1. Reality Verification and Validation
```python
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
from enum import Enum
import logging
from datetime import datetime
import asyncio

class VerificationLevel(Enum):
    UNVERIFIED = "unverified"
    PROBABLY_FALSE = "probably_false"
    UNLIKELY = "unlikely"
    POSSIBLE = "possible"
    LIKELY = "likely"
    VERIFIED = "verified"

@dataclass
class RealityCheck:
    claim: str
    verification_level: VerificationLevel
    confidence: float
    evidence: List[Dict]
    timestamp: datetime
    source: str

class RealityVerifier:
    def __init__(self):
        self.verification_rules = {}
        self.physical_constraints = self._initialize_physical_constraints()
        self.sensor_validation_matrix = {}
        self.hallucination_history = []

    def _initialize_physical_constraints(self) -> Dict[str, Tuple[float, float]]:
        """Initialize physical constraints for reality verification"""
        return {
            'velocity_max': (0.0, 5.0),  # m/s for wheeled robots
            'acceleration_max': (0.0, 10.0),  # m/s²
            'temperature_operational': (-10.0, 60.0),  # Celsius
            'voltage_range': (10.0, 14.4),  # Volts for 12V systems
            'current_max': (0.0, 20.0),  # Amperes
            'position_bounds': (-100.0, 100.0),  # meters in workspace
            'joint_angle_range': (-3.14, 3.14),  # radians
            'force_max': (0.0, 500.0)  # Newtons
        }

    def verify_claim(self, claim: str, evidence_sources: List[Dict]) -> RealityCheck:
        """Verify a claim against reality constraints"""
        verification_result = self._apply_verification_rules(claim, evidence_sources)

        reality_check = RealityCheck(
            claim=claim,
            verification_level=verification_result['level'],
            confidence=verification_result['confidence'],
            evidence=verification_result['evidence'],
            timestamp=datetime.now(),
            source=verification_result['source']
        )

        self.hallucination_history.append(reality_check)
        return reality_check

    def _apply_verification_rules(self, claim: str, evidence_sources: List[Dict]) -> Dict:
        """Apply various verification rules to assess claim validity"""
        # Parse claim for verification targets
        verification_targets = self._extract_verification_targets(claim)

        total_confidence = 0.0
        evidence_collected = []
        verification_level = VerificationLevel.UNVERIFIED

        for target in verification_targets:
            target_result = self._verify_target(target, evidence_sources)
            total_confidence += target_result['confidence']
            evidence_collected.extend(target_result['evidence'])

            # Update overall verification level based on target results
            if target_result['level'] == VerificationLevel.PROBABLY_FALSE:
                verification_level = VerificationLevel.PROBABLY_FALSE
                break  # If any target is probably false, whole claim is probably false
            elif target_result['level'] == VerificationLevel.UNLIKELY and verification_level != VerificationLevel.PROBABLY_FALSE:
                verification_level = VerificationLevel.UNLIKELY
            elif target_result['level'] == VerificationLevel.POSSIBLE and verification_level not in [VerificationLevel.PROBABLY_FALSE, VerificationLevel.UNLIKELY]:
                verification_level = VerificationLevel.POSSIBLE
            elif target_result['level'] == VerificationLevel.LIKELY and verification_level not in [VerificationLevel.PROBABLY_FALSE, VerificationLevel.UNLIKELY, VerificationLevel.POSSIBLE]:
                verification_level = VerificationLevel.LIKELY
            elif target_result['level'] == VerificationLevel.VERIFIED:
                verification_level = VerificationLevel.VERIFIED

        avg_confidence = total_confidence / max(1, len(verification_targets))

        return {
            'level': verification_level,
            'confidence': avg_confidence,
            'evidence': evidence_collected,
            'source': 'reality_verifier'
        }

    def _extract_verification_targets(self, claim: str) -> List[Dict]:
        """Extract specific targets for verification from claim"""
        targets = []

        # Extract numerical values with units
        import re
        number_patterns = [
            (r'(\d+\.?\d*)\s*(m|meter|meters)', 'distance'),
            (r'(\d+\.?\d*)\s*(s|second|seconds)', 'time'),
            (r'(\d+\.?\d*)\s*(kg|kilogram|kilograms)', 'mass'),
            (r'(\d+\.?\d*)\s*(N|newton|newtons)', 'force'),
            (r'(\d+\.?\d*)\s*(V|volt|volts)', 'voltage'),
            (r'(\d+\.?\d*)\s*(A|amp|amps|amperes)', 'current'),
            (r'(\d+\.?\d*)\s*(°C|C|celsius)', 'temperature'),
            (r'(\d+\.?\d*)\s*(m/s)', 'velocity'),
            (r'(\d+\.?\d*)\s*(m/s²)', 'acceleration'),
        ]

        for pattern, target_type in number_patterns:
            matches = re.findall(pattern, claim, re.IGNORECASE)
            for match, unit in matches:
                targets.append({
                    'type': target_type,
                    'value': float(match),
                    'unit': unit,
                    'original_text': f"{match} {unit}"
                })

        # Extract spatial relationships
        spatial_patterns = [
            (r'left|right|front|back|above|below|near|far', 'spatial'),
            (r'between|inside|outside|on top of|under', 'spatial_relationship'),
        ]

        for pattern, target_type in spatial_patterns:
            matches = re.findall(pattern, claim, re.IGNORECASE)
            for match in matches:
                targets.append({
                    'type': target_type,
                    'value': match,
                    'original_text': match
                })

        return targets

    def _verify_target(self, target: Dict, evidence_sources: List[Dict]) -> Dict:
        """Verify a specific target against physical constraints and evidence"""
        if target['type'] in ['distance', 'time', 'mass', 'force', 'voltage', 'current', 'temperature', 'velocity', 'acceleration']:
            return self._verify_physical_quantity(target, evidence_sources)
        elif target['type'] in ['spatial', 'spatial_relationship']:
            return self._verify_spatial_target(target, evidence_sources)
        else:
            return {
                'level': VerificationLevel.POSSIBLE,
                'confidence': 0.5,
                'evidence': [{'type': 'unverified', 'target': target}]
            }

    def _verify_physical_quantity(self, target: Dict, evidence_sources: List[Dict]) -> Dict:
        """Verify physical quantity against constraints"""
        constraint_key = f"{target['type']}_max" if target['type'] in ['velocity', 'acceleration', 'current', 'force'] else f"{target['type']}_range"

        if constraint_key in self.physical_constraints:
            min_val, max_val = self.physical_constraints[constraint_key]
            value = target['value']

            if value < min_val or value > max_val:
                return {
                    'level': VerificationLevel.PROBABLY_FALSE,
                    'confidence': 0.9,
                    'evidence': [{
                        'type': 'constraint_violation',
                        'target': target,
                        'constraint': (min_val, max_val),
                        'violation': f"Value {value} outside range [{min_val}, {max_val}]"
                    }]
                }
            elif value < min_val * 1.1 or value > max_val * 0.9:  # Close to limits
                return {
                    'level': VerificationLevel.UNLIKELY,
                    'confidence': 0.7,
                    'evidence': [{
                        'type': 'limit_approach',
                        'target': target,
                        'constraint': (min_val, max_val),
                        'value': value
                    }]
                }
            else:
                return {
                    'level': VerificationLevel.LIKELY,
                    'confidence': 0.8,
                    'evidence': [{
                        'type': 'constraint_compliant',
                        'target': target,
                        'constraint': (min_val, max_val),
                        'value': value
                    }]
                }

        return {
            'level': VerificationLevel.POSSIBLE,
            'confidence': 0.6,
            'evidence': [{'type': 'no_constraint', 'target': target}]
        }

    def _verify_spatial_target(self, target: Dict, evidence_sources: List[Dict]) -> Dict:
        """Verify spatial relationships using sensor data"""
        # Check sensor evidence for spatial claims
        sensor_evidence = []
        for source in evidence_sources:
            if source.get('type') == 'sensor' and 'spatial_data' in source:
                sensor_evidence.append(source)

        if not sensor_evidence:
            return {
                'level': VerificationLevel.UNLIKELY,
                'confidence': 0.3,
                'evidence': [{'type': 'no_sensor_evidence', 'target': target}]
            }

        # Analyze sensor data for spatial consistency
        consistency_score = 0
        total_sensors = len(sensor_evidence)

        for sensor in sensor_evidence:
            spatial_data = sensor.get('spatial_data', {})
            if self._check_spatial_consistency(target, spatial_data):
                consistency_score += 1

        consistency_ratio = consistency_score / total_sensors if total_sensors > 0 else 0

        if consistency_ratio >= 0.8:
            return {
                'level': VerificationLevel.VERIFIED,
                'confidence': 0.9,
                'evidence': [{'type': 'sensor_consistent', 'target': target, 'ratio': consistency_ratio}]
            }
        elif consistency_ratio >= 0.5:
            return {
                'level': VerificationLevel.LIKELY,
                'confidence': 0.7,
                'evidence': [{'type': 'partially_consistent', 'target': target, 'ratio': consistency_ratio}]
            }
        else:
            return {
                'level': VerificationLevel.UNLIKELY,
                'confidence': 0.4,
                'evidence': [{'type': 'inconsistent', 'target': target, 'ratio': consistency_ratio}]
            }

    def _check_spatial_consistency(self, target: Dict, spatial_data: Dict) -> bool:
        """Check if spatial data is consistent with target claim"""
        # This is a simplified implementation
        # In practice, this would use geometric reasoning and sensor fusion
        target_value = target['value'].lower()

        if target_value in ['left', 'right', 'front', 'back']:
            # Check relative position data
            position_ref = spatial_data.get('reference_position', {})
            object_pos = spatial_data.get('object_position', {})

            # Simple relative positioning check
            dx = object_pos.get('x', 0) - position_ref.get('x', 0)
            dy = object_pos.get('y', 0) - position_ref.get('y', 0)

            if target_value == 'left' and dx < 0:
                return True
            elif target_value == 'right' and dx > 0:
                return True
            elif target_value == 'front' and dy > 0:
                return True
            elif target_value == 'back' and dy < 0:
                return True

        return False

class SensorFusionValidator:
    def __init__(self):
        self.sensor_readings_history = {}
        self.fusion_algorithms = {}
        self.disagreement_thresholds = {
            'position': 0.1,  # 10cm
            'orientation': 0.1,  # 0.1 radian
            'temperature': 2.0,  # 2 degrees C
            'distance': 0.05  # 5cm
        }

    def validate_sensor_readings(self, sensor_readings: Dict[str, Any]) -> Dict[str, Any]:
        """Validate sensor readings using cross-validation"""
        validation_results = {}

        # Group readings by type for cross-validation
        grouped_readings = self._group_readings_by_type(sensor_readings)

        for reading_type, readings in grouped_readings.items():
            if len(readings) > 1:  # Need multiple sensors for cross-validation
                validation_results[reading_type] = self._cross_validate_readings(reading_type, readings)
            else:
                validation_results[reading_type] = self._single_sensor_validation(reading_type, readings[0])

        return validation_results

    def _group_readings_by_type(self, sensor_readings: Dict[str, Any]) -> Dict[str, List[Dict]]:
        """Group sensor readings by type"""
        grouped = {}

        for sensor_name, reading in sensor_readings.items():
            # Determine reading type from sensor name or metadata
            reading_type = self._infer_reading_type(sensor_name, reading)

            if reading_type not in grouped:
                grouped[reading_type] = []

            grouped[reading_type].append({
                'sensor': sensor_name,
                'reading': reading,
                'timestamp': datetime.now()
            })

        return grouped

    def _infer_reading_type(self, sensor_name: str, reading: Any) -> str:
        """Infer reading type from sensor name"""
        sensor_lower = sensor_name.lower()

        if any(keyword in sensor_lower for keyword in ['lidar', 'ultrasonic', 'distance', 'range']):
            return 'distance'
        elif any(keyword in sensor_lower for keyword in ['imu', 'gyro', 'accel', 'orientation']):
            return 'orientation'
        elif any(keyword in sensor_lower for keyword in ['gps', 'position', 'pose']):
            return 'position'
        elif any(keyword in sensor_lower for keyword in ['temp', 'temperature']):
            return 'temperature'
        elif any(keyword in sensor_lower for keyword in ['camera', 'vision', 'image']):
            return 'vision'
        else:
            return 'general'

    def _cross_validate_readings(self, reading_type: str, readings: List[Dict]) -> Dict:
        """Cross-validate multiple readings of the same type"""
        values = [r['reading'] for r in readings]

        # Calculate statistics
        mean_value = np.mean(values)
        std_dev = np.std(values)
        max_diff = max(abs(v - mean_value) for v in values) if values else 0

        threshold = self.disagreement_thresholds.get(reading_type, 1.0)

        if std_dev > threshold:
            # High disagreement indicates potential hallucination
            suspicious_readings = []
            for i, reading in enumerate(readings):
                if abs(reading['reading'] - mean_value) > threshold:
                    suspicious_readings.append(reading['sensor'])

            return {
                'status': 'disagreement_detected',
                'mean': mean_value,
                'std_dev': std_dev,
                'suspicious_sensors': suspicious_readings,
                'confidence': 0.3,
                'recommended_value': mean_value
            }
        else:
            # Readings are consistent
            return {
                'status': 'consistent',
                'mean': mean_value,
                'std_dev': std_dev,
                'confidence': 0.9,
                'validated_value': mean_value
            }

    def _single_sensor_validation(self, reading_type: str, reading: Dict) -> Dict:
        """Validate single sensor reading"""
        # For single readings, check against historical data and physical constraints
        sensor_name = reading['sensor']
        current_value = reading['reading']

        # Check against historical data
        historical_values = self.sensor_readings_history.get(sensor_name, [])
        if historical_values:
            historical_mean = np.mean(historical_values)
            historical_std = np.std(historical_values)

            if abs(current_value - historical_mean) > 3 * historical_std:
                # Value is 3 standard deviations from historical mean
                return {
                    'status': 'outlier_detected',
                    'historical_mean': historical_mean,
                    'historical_std': historical_std,
                    'current_value': current_value,
                    'confidence': 0.4,
                    'recommended_value': historical_mean
                }

        return {
            'status': 'validated',
            'value': current_value,
            'confidence': 0.7
        }
```

### 2. Uncertainty Quantification and Management
```python
import scipy.stats as stats
from scipy.spatial.distance import mahalanobis
import math

class UncertaintyQuantifier:
    def __init__(self):
        self.uncertainty_models = {}
        self.confidence_thresholds = {
            'low': 0.3,
            'medium': 0.7,
            'high': 0.9
        }

    def quantify_uncertainty(self, prediction: Dict[str, Any],
                           model_metadata: Dict[str, Any]) -> Dict[str, float]:
        """Quantify uncertainty in AI predictions"""
        uncertainty_metrics = {}

        # Aleatoric uncertainty (data uncertainty)
        uncertainty_metrics['aleatoric'] = self._calculate_aleatoric_uncertainty(
            prediction, model_metadata
        )

        # Epistemic uncertainty (model uncertainty)
        uncertainty_metrics['epistemic'] = self._calculate_epistemic_uncertainty(
            prediction, model_metadata
        )

        # Total uncertainty
        uncertainty_metrics['total'] = (
            uncertainty_metrics['aleatoric'] + uncertainty_metrics['epistemic']
        ) / 2

        # Confidence score (inverse of uncertainty)
        uncertainty_metrics['confidence'] = max(0, 1 - uncertainty_metrics['total'])

        return uncertainty_metrics

    def _calculate_aleatoric_uncertainty(self, prediction: Dict, metadata: Dict) -> float:
        """Calculate aleatoric (data-driven) uncertainty"""
        # For regression tasks, use prediction variance
        if 'variance' in prediction:
            variance = prediction['variance']
            # Normalize variance based on expected range
            expected_range = metadata.get('expected_range', (0, 10))
            range_width = expected_range[1] - expected_range[0]
            normalized_variance = variance / (range_width ** 2)
            return min(1.0, normalized_variance)  # Cap at 1.0

        # For classification tasks, use entropy of prediction probabilities
        if 'probabilities' in prediction:
            probs = np.array(prediction['probabilities'])
            entropy = -np.sum(probs * np.log(probs + 1e-8))  # Add small epsilon to avoid log(0)
            max_entropy = -np.log(1.0 / len(probs))  # Maximum possible entropy
            return entropy / max_entropy if max_entropy > 0 else 0

        # Default: low uncertainty if no specific information
        return 0.1

    def _calculate_epistemic_uncertainty(self, prediction: Dict, metadata: Dict) -> float:
        """Calculate epistemic (model-driven) uncertainty"""
        # Check model confidence intervals
        if 'confidence_interval' in prediction:
            lower, upper = prediction['confidence_interval']
            prediction_value = prediction.get('prediction', (lower + upper) / 2)
            interval_width = upper - lower
            expected_range = metadata.get('expected_range', (0, 10))
            range_width = expected_range[1] - expected_range[0]
            normalized_width = interval_width / range_width if range_width > 0 else 0
            return min(1.0, normalized_width)

        # Check for out-of-distribution detection
        if 'ood_score' in prediction:
            ood_score = prediction['ood_score']  # Higher score = more out-of-distribution
            return min(1.0, ood_score)

        # Default: use model metadata if available
        model_uncertainty = metadata.get('model_uncertainty', 0.1)
        return model_uncertainty

    def assess_prediction_reliability(self, prediction: Dict, uncertainty: Dict) -> Dict:
        """Assess overall reliability of prediction"""
        confidence = uncertainty['confidence']

        if confidence >= self.confidence_thresholds['high']:
            reliability = 'high'
            action = 'accept_prediction'
        elif confidence >= self.confidence_thresholds['medium']:
            reliability = 'medium'
            action = 'verify_prediction'
        else:
            reliability = 'low'
            action = 'reject_prediction'

        return {
            'reliability': reliability,
            'confidence': confidence,
            'action': action,
            'uncertainty_breakdown': uncertainty
        }

class BayesianRoboticsEstimator:
    def __init__(self):
        self.priors = {}
        self.likelihood_models = {}
        self.posterior_distributions = {}

    def update_belief(self, observation: Dict, prior_belief: Dict) -> Dict:
        """Update belief using Bayesian inference"""
        # Calculate likelihood of observation given prior
        likelihood = self._calculate_likelihood(observation, prior_belief)

        # Apply Bayes' rule: P(hypothesis|observation) = P(observation|hypothesis) * P(hypothesis) / P(observation)
        posterior = {
            'mean': (likelihood['mean'] * prior_belief.get('variance', 1) +
                    prior_belief.get('mean', 0) * likelihood.get('variance', 1)) /
                    (prior_belief.get('variance', 1) + likelihood.get('variance', 1)),
            'variance': (prior_belief.get('variance', 1) * likelihood.get('variance', 1)) /
                       (prior_belief.get('variance', 1) + likelihood.get('variance', 1)),
            'confidence': 1 - (prior_belief.get('variance', 1) * likelihood.get('variance', 1)) /
                         (prior_belief.get('variance', 1) + likelihood.get('variance', 1))
        }

        return posterior

    def _calculate_likelihood(self, observation: Dict, prior_belief: Dict) -> Dict:
        """Calculate likelihood of observation"""
        # This is a simplified likelihood model
        # In practice, this would use sensor models and physical constraints
        observation_value = observation.get('value', 0)
        expected_value = prior_belief.get('mean', 0)
        sensor_noise = observation.get('sensor_noise', 0.1)

        # Likelihood is Gaussian centered at expected value
        likelihood_mean = observation_value
        likelihood_variance = sensor_noise ** 2

        return {
            'mean': likelihood_mean,
            'variance': likelihood_variance
        }

    def detect_anomalous_observations(self, observations: List[Dict],
                                    threshold: float = 0.05) -> List[Dict]:
        """Detect anomalous observations that might indicate hallucinations"""
        anomalies = []

        for i, obs in enumerate(observations):
            # Calculate probability of observation under current belief
            belief_mean = self.posterior_distributions.get('mean', 0)
            belief_variance = self.posterior_distributions.get('variance', 1)

            if belief_variance > 0:
                # Calculate z-score
                z_score = abs(obs.get('value', 0) - belief_mean) / math.sqrt(belief_variance)
                # Calculate probability (p-value)
                p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))

                if p_value < threshold:
                    anomalies.append({
                        'index': i,
                        'observation': obs,
                        'z_score': z_score,
                        'p_value': p_value,
                        'is_anomaly': True
                    })

        return anomalies
```

### 3. Consistency Checking and Constraint Validation
```python
class ConstraintValidator:
    def __init__(self):
        self.logical_constraints = {}
        self.physical_constraints = {}
        self.temporal_constraints = {}
        self.spatial_constraints = {}

    def add_constraint(self, constraint_type: str, constraint_id: str,
                      constraint_func: callable):
        """Add a constraint to the validator"""
        constraint_store = getattr(self, f"{constraint_type}_constraints", {})
        constraint_store[constraint_id] = constraint_func

    def validate_plan_consistency(self, plan: List[Dict], context: Dict) -> Dict:
        """Validate plan consistency against constraints"""
        violations = []
        warnings = []

        for i, action in enumerate(plan):
            # Check logical constraints
            logical_violations = self._check_logical_constraints(action, plan, i, context)
            violations.extend([{'type': 'logical', 'action': i, 'violation': v}
                             for v in logical_violations])

            # Check physical constraints
            physical_violations = self._check_physical_constraints(action, context)
            violations.extend([{'type': 'physical', 'action': i, 'violation': v}
                             for v in physical_violations])

            # Check temporal constraints
            temporal_violations = self._check_temporal_constraints(action, plan, i, context)
            violations.extend([{'type': 'temporal', 'action': i, 'violation': v}
                             for v in temporal_violations])

            # Check spatial constraints
            spatial_violations = self._check_spatial_constraints(action, context)
            violations.extend([{'type': 'spatial', 'action': i, 'violation': v}
                             for v in spatial_violations])

        # Check global plan consistency
        global_violations = self._check_global_consistency(plan, context)
        violations.extend([{'type': 'global', 'violation': v}
                         for v in global_violations])

        return {
            'valid': len(violations) == 0,
            'violations': violations,
            'warnings': warnings,
            'confidence': 1.0 - min(1.0, len(violations) / len(plan)) if plan else 1.0
        }

    def _check_logical_constraints(self, action: Dict, plan: List[Dict],
                                 action_index: int, context: Dict) -> List[str]:
        """Check logical consistency of action"""
        violations = []

        # Check for contradictory actions
        if action.get('type') == 'move_to' and action_index > 0:
            previous_action = plan[action_index - 1]
            if (previous_action.get('type') == 'move_to' and
                previous_action.get('target') == action.get('target')):
                violations.append(f"Redundant move to same location")

        # Check for impossible sequences
        if action.get('type') == 'grasp' and action_index > 0:
            previous_action = plan[action_index - 1]
            if (previous_action.get('type') != 'approach' or
                previous_action.get('target') != action.get('target')):
                violations.append(f"Grasping without prior approach")

        return violations

    def _check_physical_constraints(self, action: Dict, context: Dict) -> List[str]:
        """Check physical constraints of action"""
        violations = []

        # Check velocity constraints
        max_velocity = self.physical_constraints.get('max_velocity', 1.0)
        requested_velocity = action.get('velocity', 0.5)
        if requested_velocity > max_velocity:
            violations.append(f"Requested velocity {requested_velocity} exceeds maximum {max_velocity}")

        # Check force constraints
        max_force = self.physical_constraints.get('max_force', 50.0)
        requested_force = action.get('force', 0.0)
        if requested_force > max_force:
            violations.append(f"Requested force {requested_force} exceeds maximum {max_force}")

        # Check joint angle constraints
        if 'joint_angles' in action:
            for i, angle in enumerate(action['joint_angles']):
                if abs(angle) > 3.14:  # Beyond reasonable joint limits
                    violations.append(f"Joint {i} angle {angle} exceeds physical limits")

        return violations

    def _check_temporal_constraints(self, action: Dict, plan: List[Dict],
                                  action_index: int, context: Dict) -> List[str]:
        """Check temporal consistency of action"""
        violations = []

        # Check for impossible timing
        if 'duration' in action and action['duration'] <= 0:
            violations.append(f"Action has non-positive duration: {action['duration']}")

        # Check for timing conflicts with previous actions
        if action_index > 0:
            previous_action = plan[action_index - 1]
            if (previous_action.get('duration', 0) + action.get('start_delay', 0) >
                action.get('duration', 1)):
                violations.append(f"Timing conflict: previous action duration exceeds current action time")

        return violations

    def _check_spatial_constraints(self, action: Dict, context: Dict) -> List[str]:
        """Check spatial constraints of action"""
        violations = []

        # Check if target position is within workspace bounds
        if 'target_position' in action:
            pos = action['target_position']
            workspace_bounds = context.get('workspace_bounds',
                                         {'min': [-10, -10, -10], 'max': [10, 10, 10]})

            for i, coord in enumerate(['x', 'y', 'z']):
                if coord in pos:
                    if (pos[coord] < workspace_bounds['min'][i] or
                        pos[coord] > workspace_bounds['max'][i]):
                        violations.append(f"Target position {coord}={pos[coord]} outside workspace bounds")

        # Check for collision with known obstacles
        if 'target_position' in action and 'obstacles' in context:
            target = action['target_position']
            for obstacle in context['obstacles']:
                if self._is_collision(target, obstacle):
                    violations.append(f"Target position collides with obstacle at {obstacle['position']}")

        return violations

    def _is_collision(self, pos1: Dict, pos2: Dict, threshold: float = 0.1) -> bool:
        """Check if two positions are in collision"""
        dx = pos1.get('x', 0) - pos2.get('x', 0)
        dy = pos1.get('y', 0) - pos2.get('y', 0)
        dz = pos1.get('z', 0) - pos2.get('z', 0)

        distance = math.sqrt(dx**2 + dy**2 + dz**2)
        return distance < threshold

    def _check_global_consistency(self, plan: List[Dict], context: Dict) -> List[str]:
        """Check global consistency of the entire plan"""
        violations = []

        # Check for resource conflicts
        resources_used = {}
        for i, action in enumerate(plan):
            if 'resources' in action:
                for resource in action['resources']:
                    if resource in resources_used:
                        violations.append(f"Resource {resource} conflict between actions {resources_used[resource]} and {i}")
                    else:
                        resources_used[resource] = i

        # Check for goal consistency
        if 'goal_state' in context and 'final_state' in plan[-1] if plan else {}:
            if not self._states_consistent(context['goal_state'],
                                         plan[-1]['final_state'] if plan else {}):
                violations.append("Plan does not achieve specified goal state")

        return violations

    def _states_consistent(self, state1: Dict, state2: Dict, tolerance: float = 0.01) -> bool:
        """Check if two states are consistent within tolerance"""
        for key in state1:
            if key in state2:
                if isinstance(state1[key], (int, float)) and isinstance(state2[key], (int, float)):
                    if abs(state1[key] - state2[key]) > tolerance:
                        return False
                elif state1[key] != state2[key]:
                    return False
        return True

class HallucinationDetector:
    def __init__(self):
        self.detectors = {}
        self.detection_history = []
        self.confidence_threshold = 0.5

    def add_detector(self, name: str, detector_func: callable):
        """Add a hallucination detection function"""
        self.detectors[name] = detector_func

    def detect_hallucinations(self, ai_output: Dict, context: Dict) -> Dict:
        """Detect hallucinations in AI output using multiple detectors"""
        detection_results = {}
        hallucination_score = 0.0
        total_detectors = len(self.detectors)

        for detector_name, detector_func in self.detectors.items():
            try:
                result = detector_func(ai_output, context)
                detection_results[detector_name] = result

                if result.get('is_hallucination', False):
                    hallucination_score += 1.0
                    result['severity'] = result.get('confidence', 0.5)
            except Exception as e:
                logging.error(f"Detection error in {detector_name}: {e}")
                detection_results[detector_name] = {
                    'error': str(e),
                    'is_hallucination': False,
                    'confidence': 0.0
                }

        overall_hallucination_probability = hallucination_score / total_detectors if total_detectors > 0 else 0.0

        detection_summary = {
            'is_hallucination': overall_hallucination_probability > self.confidence_threshold,
            'hallucination_probability': overall_hallucination_probability,
            'detection_results': detection_results,
            'timestamp': datetime.now(),
            'ai_output_type': ai_output.get('type', 'unknown')
        }

        self.detection_history.append(detection_summary)
        return detection_summary

    def register_default_detectors(self):
        """Register default hallucination detection methods"""

        def context_consistency_detector(ai_output: Dict, context: Dict) -> Dict:
            """Detect hallucinations based on context consistency"""
            claim = ai_output.get('claim', '')
            context_info = context.get('known_facts', [])

            # Simple keyword matching for context consistency
            context_keywords = set()
            for fact in context_info:
                if isinstance(fact, str):
                    context_keywords.update(fact.lower().split())

            claim_keywords = set(claim.lower().split())
            intersection = context_keywords.intersection(claim_keywords)
            consistency_ratio = len(intersection) / len(claim_keywords) if claim_keywords else 0

            return {
                'is_hallucination': consistency_ratio < 0.3,
                'confidence': consistency_ratio,
                'consistency_ratio': consistency_ratio
            }

        def numerical_reasonableness_detector(ai_output: Dict, context: Dict) -> Dict:
            """Detect hallucinations in numerical claims"""
            claim = ai_output.get('claim', '')

            # Extract numerical values
            import re
            numbers = re.findall(r'\d+\.?\d*', claim)
            unreasonable_count = 0

            for num_str in numbers:
                num = float(num_str)
                # Check for obviously unreasonable values
                if num > 1000000 or (num < 0 and 'negative' not in claim.lower()):
                    unreasonable_count += 1

            unreasonable_ratio = unreasonable_count / len(numbers) if numbers else 0

            return {
                'is_hallucination': unreasonable_ratio > 0.5,
                'confidence': 1 - unreasonable_ratio,
                'unreasonable_ratio': unreasonable_ratio
            }

        def temporal_consistency_detector(ai_output: Dict, context: Dict) -> Dict:
            """Detect temporal hallucinations"""
            claim = ai_output.get('claim', '')

            # Look for time-related inconsistencies
            import re
            time_patterns = [r'(\d{1,2}:\d{2})', r'(\d+\s*(seconds?|minutes?|hours?))']

            for pattern in time_patterns:
                matches = re.findall(pattern, claim)
                # In a real implementation, validate against known time constraints
                # For now, return neutral result
                pass

            return {
                'is_hallucination': False,
                'confidence': 0.8,
                'temporal_valid': True
            }

        self.add_detector('context_consistency', context_consistency_detector)
        self.add_detector('numerical_reasonableness', numerical_reasonableness_detector)
        self.add_detector('temporal_consistency', temporal_consistency_detector)
```

### 4. Fallback Mechanisms and Safe Alternatives
```python
class FallbackManager:
    def __init__(self):
        self.fallback_strategies = {}
        self.safe_modes = {}
        self.emergency_procedures = {}
        self.current_fallback_level = 'normal'

    def register_fallback_strategy(self, condition: str, strategy_func: callable):
        """Register a fallback strategy for specific condition"""
        self.fallback_strategies[condition] = strategy_func

    def activate_fallback(self, condition: str, current_state: Dict) -> Dict:
        """Activate appropriate fallback based on condition"""
        if condition in self.fallback_strategies:
            fallback_func = self.fallback_strategies[condition]
            return fallback_func(current_state)
        else:
            # Use default safe behavior
            return self._default_safe_behavior(current_state)

    def _default_safe_behavior(self, current_state: Dict) -> Dict:
        """Default safe behavior when no specific fallback is available"""
        safe_action = {
            'type': 'safe_stop',
            'description': 'Default safe behavior - stop all non-essential operations',
            'parameters': {
                'stop_motors': True,
                'reduce_power': True,
                'maintain_communication': True,
                'log_state': current_state
            },
            'priority': 'high'
        }
        return safe_action

    def implement_safe_degradation(self, hallucination_severity: str) -> str:
        """Implement safe degradation based on hallucination severity"""
        degradation_levels = {
            'low': 'continue_with_validation',
            'medium': 'reduce_functionality',
            'high': 'safe_mode',
            'critical': 'emergency_stop'
        }

        if hallucination_severity in degradation_levels:
            new_level = degradation_levels[hallucination_severity]
            self.current_fallback_level = new_level
            return new_level
        else:
            return 'normal'

    def create_safe_navigation_fallback(self, original_path: List[Dict]) -> List[Dict]:
        """Create safe navigation fallback when path planning is unreliable"""
        # Simplified safe path that avoids known obstacles
        safe_path = []

        for waypoint in original_path:
            # Validate each waypoint against known safe corridors
            if self._is_waypoint_safe(waypoint):
                safe_path.append(waypoint)
            else:
                # Replace with nearest safe waypoint
                safe_waypoint = self._find_nearest_safe_waypoint(waypoint)
                safe_path.append(safe_waypoint)

        return safe_path

    def _is_waypoint_safe(self, waypoint: Dict) -> bool:
        """Check if waypoint is in safe area"""
        # Check against safety zones
        x, y = waypoint.get('x', 0), waypoint.get('y', 0)

        # Define safety boundaries (in practice, these would come from safety maps)
        safe_x_range = (-50, 50)
        safe_y_range = (-50, 50)

        return (safe_x_range[0] <= x <= safe_x_range[1] and
                safe_y_range[0] <= y <= safe_y_range[1])

    def _find_nearest_safe_waypoint(self, waypoint: Dict) -> Dict:
        """Find nearest safe waypoint to given position"""
        # For now, return a simple safe position
        # In practice, this would use pathfinding to safe zones
        safe_x = max(-49, min(49, waypoint.get('x', 0)))
        safe_y = max(-49, min(49, waypoint.get('y', 0)))

        return {
            'x': safe_x,
            'y': safe_y,
            'z': waypoint.get('z', 0),
            'safe_adjusted': True
        }

class GroundTruthValidator:
    def __init__(self):
        self.ground_truth_sources = {}
        self.validation_thresholds = {}
        self.trust_scores = {}

    def add_ground_truth_source(self, source_name: str, validation_func: callable,
                              trust_score: float = 0.9):
        """Add a ground truth validation source"""
        self.ground_truth_sources[source_name] = validation_func
        self.trust_scores[source_name] = trust_score

    def validate_ai_claim(self, claim: str, ai_source: str) -> Dict:
        """Validate AI claim against ground truth sources"""
        validation_results = {}
        weighted_confidence = 0.0
        total_weight = 0.0

        for source_name, validation_func in self.ground_truth_sources.items():
            try:
                result = validation_func(claim)
                trust_weight = self.trust_scores[source_name]

                validation_results[source_name] = {
                    'result': result,
                    'confidence': result.get('confidence', 0.5),
                    'trust_weight': trust_weight
                }

                weighted_confidence += result.get('confidence', 0.5) * trust_weight
                total_weight += trust_weight

            except Exception as e:
                logging.error(f"Validation error from {source_name}: {e}")
                validation_results[source_name] = {
                    'result': {'valid': False, 'error': str(e)},
                    'confidence': 0.0,
                    'trust_weight': 0.0
                }

        overall_confidence = weighted_confidence / total_weight if total_weight > 0 else 0.0

        return {
            'claim': claim,
            'ai_source': ai_source,
            'validation_results': validation_results,
            'overall_confidence': overall_confidence,
            'is_valid': overall_confidence > 0.7,  # Threshold for validity
            'timestamp': datetime.now()
        }

    def register_default_ground_truth_sources(self):
        """Register default ground truth validation sources"""

        def physics_validation(claim: str) -> Dict:
            """Validate claim against basic physics principles"""
            # Check for violations of basic physics
            if 'faster than light' in claim.lower():
                return {'valid': False, 'confidence': 0.99, 'reason': 'violates physics'}
            elif 'impossible force' in claim.lower():
                return {'valid': False, 'confidence': 0.95, 'reason': 'exceeds physical limits'}
            else:
                return {'valid': True, 'confidence': 0.8, 'reason': 'consistent with physics'}

        def geometric_validation(claim: str) -> Dict:
            """Validate spatial claims using geometric reasoning"""
            import re

            # Check for geometric impossibilities
            if re.search(r'inside.*outside', claim, re.IGNORECASE):
                return {'valid': False, 'confidence': 0.9, 'reason': 'geometric contradiction'}

            # Check for dimensional inconsistencies
            if '2D object' in claim and 'volume' in claim:
                return {'valid': False, 'confidence': 0.8, 'reason': 'dimensional inconsistency'}

            return {'valid': True, 'confidence': 0.7, 'reason': 'geometrically possible'}

        def temporal_validation(claim: str) -> Dict:
            """Validate temporal claims"""
            # Check for temporal impossibilities
            if 'time travel' in claim.lower():
                return {'valid': False, 'confidence': 0.95, 'reason': 'temporal impossibility'}

            return {'valid': True, 'confidence': 0.75, 'reason': 'temporally consistent'}

        self.add_ground_truth_source('physics', physics_validation, 0.95)
        self.add_ground_truth_source('geometry', geometric_validation, 0.85)
        self.add_ground_truth_source('temporal', temporal_validation, 0.8)
```

## Best Practices for Hallucination Handling in Robotics

### 1. Multi-layered Validation Approach
```python
class MultiLayerValidator:
    def __init__(self):
        self.layer_validators = [
            self._sensor_validation_layer,
            self._constraint_validation_layer,
            self._consistency_validation_layer,
            self._ground_truth_validation_layer
        ]

    def validate_input(self, input_data: Dict) -> Dict:
        """Validate input through multiple layers"""
        validation_report = {
            'input': input_data,
            'layers': {},
            'overall_validity': True,
            'confidence': 1.0
        }

        for i, validator in enumerate(self.layer_validators):
            layer_name = f"layer_{i}"
            result = validator(input_data)
            validation_report['layers'][layer_name] = result
            validation_report['overall_validity'] &= result.get('valid', True)
            validation_report['confidence'] *= result.get('confidence', 1.0)

        return validation_report

    def _sensor_validation_layer(self, data: Dict) -> Dict:
        """First layer: Sensor data validation"""
        # Validate sensor readings using sensor fusion
        sensor_validator = SensorFusionValidator()
        results = sensor_validator.validate_sensor_readings(data.get('sensors', {}))

        valid_count = sum(1 for v in results.values() if v.get('status') in ['consistent', 'validated'])
        total_count = len(results)
        confidence = valid_count / total_count if total_count > 0 else 1.0

        return {
            'valid': confidence > 0.5,
            'confidence': confidence,
            'details': results
        }

    def _constraint_validation_layer(self, data: Dict) -> Dict:
        """Second layer: Constraint validation"""
        # Validate against physical and logical constraints
        constraint_validator = ConstraintValidator()
        plan = data.get('plan', [])
        context = data.get('context', {})

        validation_result = constraint_validator.validate_plan_consistency(plan, context)

        return {
            'valid': validation_result['valid'],
            'confidence': validation_result['confidence'],
            'details': validation_result
        }

    def _consistency_validation_layer(self, data: Dict) -> Dict:
        """Third layer: Consistency validation"""
        # Check internal consistency of the data
        consistency_issues = []

        # Check if goals are achievable given current state
        current_state = data.get('current_state', {})
        goal_state = data.get('goal_state', {})

        if not self._is_goal_achievable(current_state, goal_state):
            consistency_issues.append("Goal not achievable from current state")

        # Check temporal consistency
        if not self._is_temporally_consistent(data):
            consistency_issues.append("Temporal inconsistencies detected")

        return {
            'valid': len(consistency_issues) == 0,
            'confidence': 1.0 - min(1.0, len(consistency_issues) * 0.2),
            'details': consistency_issues
        }

    def _ground_truth_validation_layer(self, data: Dict) -> Dict:
        """Fourth layer: Ground truth validation"""
        # Validate against known ground truth sources
        ground_truth_validator = GroundTruthValidator()
        ground_truth_validator.register_default_ground_truth_sources()

        claim = data.get('claim', str(data))
        result = ground_truth_validator.validate_ai_claim(claim, 'robot_ai_system')

        return {
            'valid': result['is_valid'],
            'confidence': result['overall_confidence'],
            'details': result
        }

    def _is_goal_achievable(self, current: Dict, goal: Dict) -> bool:
        """Check if goal is achievable from current state"""
        # Simplified check - in practice, this would use planning algorithms
        return True  # Placeholder

    def _is_temporally_consistent(self, data: Dict) -> bool:
        """Check temporal consistency of data"""
        # Placeholder implementation
        return True
```

### 2. Real-time Hallucination Detection
```python
class RealTimeHallucinationDetector:
    def __init__(self):
        self.hallucination_detector = HallucinationDetector()
        self.hallucination_detector.register_default_detectors()
        self.validation_buffer = []
        self.detection_window = 10  # Number of recent validations to consider

    async def detect_hallucinations_realtime(self, ai_output: Dict, context: Dict) -> Dict:
        """Detect hallucinations in real-time"""
        detection_result = self.hallucination_detector.detect_hallucinations(ai_output, context)

        # Add to validation buffer
        self.validation_buffer.append(detection_result)
        if len(self.validation_buffer) > self.detection_window:
            self.validation_buffer.pop(0)

        # Calculate recent hallucination rate
        recent_hallucinations = sum(1 for r in self.validation_buffer if r['is_hallucination'])
        hallucination_rate = recent_hallucinations / len(self.validation_buffer) if self.validation_buffer else 0

        # Determine response based on hallucination rate
        if hallucination_rate > 0.7:
            response_level = 'emergency_stop'
        elif hallucination_rate > 0.3:
            response_level = 'cautious_mode'
        elif hallucination_rate > 0.1:
            response_level = 'increased_validation'
        else:
            response_level = 'normal_operation'

        return {
            **detection_result,
            'recent_hallucination_rate': hallucination_rate,
            'response_level': response_level,
            'recommended_action': self._get_recommended_action(response_level)
        }

    def _get_recommended_action(self, response_level: str) -> str:
        """Get recommended action based on response level"""
        actions = {
            'emergency_stop': 'Stop all autonomous operations, switch to manual control',
            'cautious_mode': 'Reduce operation speed, increase validation frequency',
            'increased_validation': 'Run additional validation checks on all outputs',
            'normal_operation': 'Continue normal operations'
        }
        return actions.get(response_level, 'unknown')
```

## Best Practices
- Implement multi-layered validation to catch hallucinations at different levels
- Use sensor fusion to cross-validate AI-generated information
- Quantify and track uncertainty in AI predictions
- Establish clear fallback mechanisms when hallucinations are detected
- Maintain ground truth references for validation
- Implement real-time hallucination detection and response
- Use constraint validation to ensure physical plausibility
- Regularly update validation models based on new data
- Log hallucination events for system improvement
- Design graceful degradation strategies for different hallucination severities